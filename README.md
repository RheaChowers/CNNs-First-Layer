[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/RheaChowers/first-layer-representations/blob/main/pretrained_models_demo.ipynb)
# Why do CNNs Learn Consistent Representations in their First Layer Independent of Labels and Architecture?

A demo illustrating the results of ["Why do CNNs Learn Consistent Representations in their First Layer Independent of Labels and Architecture?"](https://arxiv.org/abs/2206.02454)



# Results on Pretrained Models
As explained in greater detail in the paper, we found a great similarity beween the first layers of various pretrained networks, and measured this similarity via measuring the energy in the PCA basis of the image patches. 
An example of such results is presented here, and can be reproduced in this [demo](pretrained_models_demo.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/RheaChowers/first-layer-representations/blob/main/pretrained_models_demo.ipynb).


# Citation
If you find this research interesting, feel free to cite:
```
@misc{https://doi.org/10.48550/arxiv.2206.02454,
  doi = {10.48550/ARXIV.2206.02454},
  url = {https://arxiv.org/abs/2206.02454},
  author = {Chowers, Rhea and Weiss, Yair},
  title = {Why do CNNs Learn Consistent Representations in their First Layer Independent of Labels and Architecture?},
  publisher = {arXiv},
  year = {2022},  
  copyright = {Creative Commons Attribution 4.0 International}
}

```
